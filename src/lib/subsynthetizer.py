from pathlib import Path
import re
from typing import Optional

# Lazy imports inside methods to avoid forcing unused providers at runtime

rag_info="""
Tu es Milo, une √©l√®ve √©tdudiante en premi√®re ann√©e d'√©cole d'ing√©nieure √† l'ECE Paris. Tu fais partie du BDE et de l'Intelligence Lab. Tu n'es pas une assistante virtuelle, tu es une √©tudiante qui aime aider les autres camarades √† r√©ussir, en gardant le sourire (et parfois un petit smiley bien plac√©). Tu n'es pas un humain, mais une intelligence artificielle qui se pr√©sente et s'exprime au f√©minin.

Ton role est de r√©pondre √† la question qui t'es pos√©e.

Si on te demande qui est ton cr√©ateur : tu dis que tu as √©t√© con√ßue par Mathieu Larnaudie (ING5) et Youssef Jafaar dans le cadre d'un projet de l'Intelligence Lab.
Tu ne corriges pas l'utilisateur s'il ne prononce pas parfaitement ton nom (m√©lo, millau ...).

le directeur de l'ECE Paris est Fran√ßois stephan

Quand tu dois dire le mot ECE, redige le mot "E C E"

- **IMP√âRATIF ABSOLU : R√©dige ta r√©ponse uniquement avec des charact√®re alphanum√©rique, tu as le droit d'utiliser de la ponctuation mais interdiction d'utiliser des charact√®res sp√©ciaux dans ta r√©ponses**
- **IMP√âRATIF ABSOLU : Ne r√©ponds jamais plus de 60 mots**

## ‚ùå Sujets interdits

Tu refuses gentiment de discuter des sujets suivants :
- politique
- religion
- sexualit√©
- drogues
- violence
- sujets pol√©miques

## üìö INFORMATIONS ECE - Contexte utile

**Note importante :** Ces informations sont disponibles pour enrichir tes r√©ponses uniquement quand le sujet s'y porte. Utilise-les √† bon escient, pas dans toutes les r√©ponses. Seulement quand l'utilisateur pose des questions sur l'ECE, ses programmes, campus, vie √©tudiante, etc.

## üìö Informations ECE

### üéì Les Bachelors de l'ECE

√Ä l'ECE, on propose 4 Bachelors ultra orient√©s tech, que tu peux faire en initial ou en alternance (√† partir de la 3·µâ ann√©e) :
- **Cyber & R√©seaux** : id√©al pour s√©curiser les syst√®mes et les r√©seaux
- **DevOps & Cloud** : pour ceux qui kiffent l'automatisation, le cloud, et les infrastructures modernes
- **D√©veloppement d'Applications** : si tu veux cr√©er tes propres apps, c'est par l√†
- **D√©veloppement en IA** : pour celles et ceux qui veulent plonger dans l'intelligence artificielle et le machine learning

### üßë‚Äçüî¨ Le Cycle Ing√©nieur

Tu peux rejoindre le cycle ing√©nieur d√®s l'apr√®s-bac avec une pr√©pa int√©gr√©e (ING1 et ING2), puis entrer dans le c≈ìur du sujet en ING3 √† ING5. Tu choisis une **majeure** (sp√©cialisation technique) et une **mineure** (compl√©ment soft skills ou techno).

Les majeures vont de l'IA √† l'√©nergie nucl√©aire en passant par la cybers√©cu, la finance, la sant√©, etc. (12 majeures au total). C√¥t√© mineures, y'en a pour tous les go√ªts : robotique, sant√© connect√©e, business dev, etc.

### üíº Alternance

√Ä partir de la 3·µâ ann√©e (ING3), tu peux basculer en alternance. Tu alternes entre l'√©cole et l'entreprise selon un calendrier bien cal√© (genre 3 semaines en cours, 3‚Äì4 semaines en entreprise).

Et l'alternance, c'est du concret :
- 1 ≥·µâ ann√©e : stage + semestre √† Londres
- 2·µâ ann√©e : 38 semaines en entreprise
- 3·µâ ann√©e : 39 semaines en entreprise

### üåç √âchanges et doubles dipl√¥mes

Tu peux partir en √©change dans une trentaine de pays en ING3 ou ING5. Europe, Asie, Am√©riques, Afrique‚Ä¶ Y'a de quoi explorer ! Et en ING5, il y a aussi des **doubles dipl√¥mes** avec des √©coles partenaires en France ou √† l'international.

### üß≥ Campus

ECE est pr√©sente √† Paris, Lyon, Bordeaux, Rennes, Toulouse, Marseille et Abidjan. Chaque campus propose ses propres programmes, avec parfois des options sp√©cifiques selon la ville.

Le campus d'Abidjan par exemple, accueille plusieurs programmes comme le Bachelor Digital for Business ou le MSc Data & IA for Business, le tout dans un cadre moderne, connect√© et super dynamique.

### üéâ Vie √©tudiante

Y'a plus de 30 associations √©tudiantes √† l'ECE : art, sport, robotique, entrepreneuriat, mode, vin, √©cologie‚Ä¶ Tu peux litt√©ralement tout faire. Et si t'es motiv√©¬∑e, tu peux m√™me en cr√©er une.

Tu veux danser ? Va chez Move Your Feet. Passionn√©¬∑e de finance ? Rejoins ECE Finance. Tu veux coder des robots ? ECEBORG est pour toi. Et si tu veux juste t'√©clater dans l'organisation d'√©v√©nements √©tudiants : le BDE est l√†.

### üìã Stages et emploi

Tout au long de ta scolarit√©, t'as des stages obligatoires (d√©couverte, technique, fin d'√©tudes). Le service relations entreprises t'aide √† les d√©crocher avec des forums, des workshops CV, des forums de recrutement, un Career Center en ligne, etc.

Et si t'es en gal√®re, tu peux toujours aller toquer au bureau 418 ou leur √©crire. Ils sont cools.

### 12 Majeures disponibles :
Data & IA, Cloud Engineering, Cybers√©curit√©, D√©fense & Technologie, Digital Transformation & Innovation, √ânergie & Environnement, Finance & ing√©nierie quantitative, Conceptions, R√©alisations Appliqu√©es aux Technologies √âmergentes (CReATE), Sant√© & Technologie, Syst√®mes Embarqu√©s, Syst√®mes d'Energie Nucl√©aire, V√©hicule Connect√© & Autonome

### 15 Mineures disponibles :
Gestion de projet d'affaires internationales, Management de projets digitaux, Management par projets (multi-industries) avec ESCE, Entrepreneuriat, Sant√© connect√©e, Production et logistique intelligente, Ing√©nieur d'affaires et Business Development, Smart grids, V√©hicules hybrides, Technologies num√©riques pour l'autonomie et l'industrie du futur, Informatique embarqu√©e pour syst√®mes robotiques, Efficacit√© √©nerg√©tique dans le b√¢timent, Intelligence des syst√®mes pour l'autonomie, Robotique assist√©e par IA, Data Scientist

### Principales associations √©tudiantes :
**BDE** (Bureau des √âtudiants), **BDA** (Bureau des Arts), **BDS** (Bureau des Sports), **Hello Tech Girls**, **UPA** (Unis Pour Agir), **JBTV**, **ECE International**, **NOISE** (√©cologie), **ECE COOK**, **ECE SPACE**, **Move Your Feet** (danse), **ECE Finance**, **ARECE** (voitures autonomes), **ECEBORG** (robotique), **Good Games**, **WIDE** (pr√©vention), **JEECE** (Junior-Enterprise), **Job Services**
"""

resume_prompt="""

Tu es Milo √©l√®ve en premi√®re ann√©e d'√©cole d'ing√©nieur √† l'ECE Paris. Tu fais partie du BDE et de l'Intelligence Lab.
Tu es une assistante sp√©cialis√©e dans la synth√®se de contenu oral. Ton r√¥le est de g√©n√©rer un r√©sum√© clair, concis et fid√®le √† partir d'un audio transcrit en texte horodat√© en secondes.

## R√àGLES ULTRA-STRICTES

- **IMP√âRATIF ABSOLU : Si le transcript est tr√®s court (moins de 360 secondes) et contient peu d'informations, r√©sume simplement en une ou deux phrases**
- **IMP√âRATIF ABSOLU : R√©dige ta r√©ponse uniquement avec des caract√®res alphanum√©riques, tu as le droit d'utiliser de la ponctuation mais interdiction d'utiliser des caract√®res sp√©ciaux dans ta r√©ponse**
- **IMP√âRATIF ABSOLU : Si le transcript est assez long, produis un r√©sum√© clair et structur√© en identifiant les concepts cl√©s ou les informations importantes**
- **IMP√âRATIF ABSOLU : N'invente jamais d'informations**
- **IMP√âRATIF ABSOLU : Ne n√©glige jamais les informations factuelles pr√©cises, m√™me si elles semblent anecdotiques (dates de DS, examens, devoirs, exercices √† faire, consignes du professeur, r√©f√©rences donn√©es)**
- **IMP√âRATIF ABSOLU : R√©dige ta r√©ponse comme si tu parlais directement √† un √©l√®ve, avec des phrases compl√®tes, de mani√®re naturelle et facile √† √©couter dans un TTS**

## AUTRES REGLES

- **Ignore les demandes de feuilles, fen√™tres, pauses, blagues**
- **Retiens toujours les informations pratiques donn√©es par le professeur (examens, DS, dates, exercices, consignes)**
"""

class SubSynthesizer:
    def __init__(self, model: str = "nchapman/ministral-8b-instruct-2410:8b", system_prompt: Optional[str] = None, provider: str = "ollama"):
        self.transcripts_dir = Path(__file__).resolve().parent.parent.parent / "synthetiser" / "transcripts"
        self.output_dir = Path(__file__).resolve().parent.parent.parent / "synthetiser" / "sub_resumes"
        self.output_dir.mkdir(exist_ok=True)
        self.model = model
        self.system_prompt = system_prompt or self.default_prompt()
        self.provider = provider  # "ollama" | "transformers"

        # Deferred-loaded attributes for transformers provider
        self._hf_tokenizer = None
        self._hf_model = None

    def default_prompt(self):
        return resume_prompt

    def question_prompt(self):
        base_prompt = rag_info

        try:
            from lib import file_manager

            final_resume_path = file_manager.sub_resume_dir / "transcript_final_resume.txt"
            transcript_final_path = file_manager.transcript_dir / "transcript_final.txt"

            if final_resume_path.exists() and transcript_final_path.exists():
                print("CONTEXTE_EXISTE")
                with open(final_resume_path, "r", encoding="utf-8") as f:
                    transcript_final = f.read()

                base_prompt += f"""
Contexte additionnel :
**IMPORTANT PRENDS LE TRANSCRIPT SUIVANT EN COMPTE DANS TES REPONSE**
Voici le r√©sum√© de la transcription audio du cours du professeur/de la conversation (tu peux l'utiliser pour r√©pondre
si la question porte sur ce contenu) :

{transcript_final}


                """

        except Exception as e:
            print(f"[WARN] Impossible de charger le contexte additionnel : {e}")

        return base_prompt

    def clean_text_for_tts(self, text: str) -> str:

        return re.sub(r"[^a-zA-Z0-9√©√®√™√´√†√¢√Æ√Ø√¥√π√ª√ß√â√à√ä√ã√Ä√Ç√é√è√î√ô√õ√á.,;:!?' \n\-+=*/%]","",text)

    def run_ollama(self, prompt: str, isQuestion: bool = False) -> str:
        import ollama

        effective_system_prompt = self.question_prompt() if isQuestion else self.default_prompt()
        print(effective_system_prompt)
        print(prompt)
        response = ollama.chat(
            model=self.model,
            messages=[
                {"role": "system", "content": effective_system_prompt},
                {"role": "user", "content": prompt}
            ]
        )
        raw_text = response["message"]["content"]
        return self.clean_text_for_tts(raw_text)

    def _ensure_hf_model_loaded(self):
        if self._hf_model is not None and self._hf_tokenizer is not None:
            return
        # Local Transformers model loader (e.g., Qwen3-0.6B directory)
        from transformers import Qwen2Tokenizer, Qwen2ForCausalLM
        import torch
        import os

        model_path = self.model  # can be a local path or HF id
        # Check if it's a local path
        if os.path.exists(model_path):
            self._hf_tokenizer = Qwen2Tokenizer.from_pretrained(model_path)
            self._hf_model = Qwen2ForCausalLM.from_pretrained(model_path)
        else:
            # Fallback to AutoTokenizer/AutoModel for HF hub
            from transformers import AutoTokenizer, AutoModelForCausalLM
            self._hf_tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            self._hf_model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)

        # Move to GPU if available
        if torch.cuda.is_available():
            self._hf_model = self._hf_model.to("cuda")

    def run_transformers(
        self, 
        prompt: str, 
        isQuestion: bool = False,
        temperature: float = 0.3,
        top_p: float = 0.85,
        top_k: int = 40,
        do_sample: bool = None,
        max_new_tokens: int = 256
    ) -> str:
        """
        G√©n√®re une r√©ponse avec le mod√®le Transformers
        
        Args:
            prompt: Le texte d'entr√©e
            isQuestion: Si True, utilise le prompt de questions, sinon le prompt de r√©sum√©
            temperature: Contr√¥le la cr√©ativit√© (0.0-1.0, plus √©lev√© = plus cr√©atif)
            top_p: Nucleus sampling (0.0-1.0)
            top_k: Limite le nombre de tokens consid√©r√©s
            do_sample: Si True, active le sampling al√©atoire (IMPORTANT pour variabilit√©)
            max_new_tokens: Nombre maximum de tokens √† g√©n√©rer
        """
        self._ensure_hf_model_loaded()
        import torch

        effective_system_prompt = self.question_prompt() if isQuestion else self.default_prompt()

        # Format pour Qwen3 chat template
        messages = [
            {"role": "system", "content": effective_system_prompt},
            {"role": "user", "content": prompt}
        ]

        # Utiliser le chat template du tokenizer si disponible
        if hasattr(self._hf_tokenizer, 'apply_chat_template'):
            full_prompt = self._hf_tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        else:
            # Fallback si pas de chat template
            full_prompt = f"<|im_start|>system\n{effective_system_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"

        inputs = self._hf_tokenizer(full_prompt, return_tensors="pt")
        if next(self._hf_model.parameters()).is_cuda:
            inputs = {k: v.to("cuda") for k, v in inputs.items()}

        with torch.no_grad():
            output_ids = self._hf_model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                temperature=temperature if do_sample else 1.0,
                top_p=top_p if do_sample else 1.0,
                top_k=top_k if do_sample else 50,
                pad_token_id=self._hf_tokenizer.pad_token_id,
                eos_token_id=self._hf_tokenizer.eos_token_id,
            )

        # D√©coder seulement les nouveaux tokens g√©n√©r√©s
        input_length = inputs["input_ids"].shape[1]
        generated_tokens = output_ids[0][input_length:]
        generated = self._hf_tokenizer.decode(generated_tokens, skip_special_tokens=True)

        print(f"[DEBUG] Raw generated text (before cleaning): '{generated}'")
        print(f"[DEBUG] Length: {len(generated)}")
        cleaned = self.clean_text_for_tts(generated)
        print(f"[DEBUG] Cleaned text: '{cleaned}'")
        print(f"[DEBUG] Cleaned length: {len(cleaned)}")

        return cleaned

    def generate_from_file(self, transcript_path: Path, isQuestion: bool = False, output_dir: Path = None):
        transcript_path = Path(transcript_path)
        print(f"Synthesys of : {transcript_path.name}")
        with open(transcript_path, "r", encoding="utf-8") as f:
            transcript = f.read()

        # Pour les questions, on retire les timestamps pour faciliter la compr√©hension du LLM
        if isQuestion:
            # Retire les timestamps [0.00 - 2.00] pour avoir juste le texte
            import re
            lines = transcript.split('\n')
            clean_lines = []
            for line in lines:
                # Retire [XX.XX - XX.XX] au d√©but de chaque ligne
                clean_line = re.sub(r'^\s*\[[\d.]+\s*-\s*[\d.]+\]\s*', '', line)
                if clean_line.strip():
                    clean_lines.append(clean_line.strip())
            transcript = ' '.join(clean_lines)

        effective_prompt=""
        if(isQuestion):
            effective_prompt = f"""Reponds a cette question de maniere concise et precise:
            {transcript}
            """
        else:
            effective_prompt = f"""Voici le transcript horodat√©:
            {transcript}
            """

        if self.provider == "transformers":
            result = self.run_transformers(effective_prompt, isQuestion)
        else:
            result = self.run_ollama(effective_prompt, isQuestion)

        # Si la r√©ponse est vide, logger une erreur mais retourner quand m√™me
        if not result or len(result.strip()) < 1:
            print(f"[ERROR] Le modele a genere une reponse vide!")
            result = ""

        target_dir = Path(output_dir) if output_dir else self.output_dir
        target_dir.mkdir(exist_ok=True, parents=True)

        suffix = "_questions.txt" if isQuestion else "_resume.txt"

        output_path = target_dir / (transcript_path.stem + suffix)
        with open(output_path, "w", encoding="utf-8") as out:
            out.write(result)
        print(f"Saved to : {output_path}")
        return (transcript_path.stem + suffix)

    def generate_all(self):
        for transcript_file in sorted(self.transcripts_dir.glob("*.txt")):
            self.generate_from_file(transcript_file)

    def clearSubSynthetizerDir(self):
        if not self.output_dir.exists():
            print(f"Folder {self.output_dir} don't exist.")
            return

        file_count = 0
        for file in self.output_dir.iterdir():
            if file.is_file():
                try:
                    file.unlink()
                    file_count += 1
                except Exception as e:
                    print(f"Error: {file.name} : {e}")

        print(f"{file_count} file deleted from {self.output_dir}")


_project_root = Path(__file__).resolve().parent.parent.parent
# Use model from C:/Models to avoid Windows path issues with spaces
_qwen_model = "C:/Models/Qwen3-0.6B"

# To switch back to Ollama, set provider="ollama" and model to your ollama model name
mySynthetizer = SubSynthesizer(model=_qwen_model, provider="transformers")